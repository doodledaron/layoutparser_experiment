{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“„ Fine-Tuning Complex Law Document Layout Detection in Google Colab\n",
    "\n",
    "# ---\n",
    "# Setup Info:\n",
    "# Model: Faster R-CNN R50-FPN pretrained on PubLayNet\n",
    "# Framework: Detectron2\n",
    "# Goal: Fine-tune to handle multi-column, tables, footnotes, headers, signatures, logos.\n",
    "\n",
    "# ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: Pillow==9.5.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (9.5.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install \"Pillow==9.5.0\" #use a downgrade version of PIL\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: layoutparser\n",
      "Version: 0.3.4\n",
      "Summary: A unified toolkit for Deep Learning Based Document Image Analysis\n",
      "Home-page: https://github.com/Layout-Parser/layout-parser\n",
      "Author: Zejiang Shen, Ruochen Zhang, and Layout Parser Model Contributors\n",
      "Author-email: layoutparser@gmail.com\n",
      "License: Apache-2.0\n",
      "Location: /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages\n",
      "Requires: iopath, numpy, opencv-python, pandas, pdf2image, pdfplumber, pillow, pyyaml, scipy\n",
      "Required-by: \n",
      "---\n",
      "Name: Pillow\n",
      "Version: 9.5.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.org\n",
      "Author: Jeffrey A. Clark (Alex)\n",
      "Author-email: aclark@aclark.net\n",
      "License: HPND\n",
      "Location: /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages\n",
      "Requires: \n",
      "Required-by: detectron2, fvcore, layoutparser, matplotlib, pdf2image, pdfplumber, pytesseract, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show layoutparser pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting detectron2\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.4) to /private/var/folders/57/0byy2pcx5fnckzr0s35zly7w0000gn/T/pip-install-r70qnbz1/detectron2_56eff9c5783440758026371bea81fc62\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /private/var/folders/57/0byy2pcx5fnckzr0s35zly7w0000gn/T/pip-install-r70qnbz1/detectron2_56eff9c5783440758026371bea81fc62\n",
      "  Running command git checkout -q 4aca4bdaa9ad48b8e91d7520e0d0815bb8ca0fb1\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 4aca4bdaa9ad48b8e91d7520e0d0815bb8ca0fb1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.0.1)\n",
      "Requirement already satisfied: Pillow>=7.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (9.5.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.1.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.10.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (4.67.1)\n",
      "Requirement already satisfied: tensorboard in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (2.19.0)\n",
      "Requirement already satisfied: fvcore<0.1.4,>=0.1.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.1.3.post20210317)\n",
      "Requirement already satisfied: iopath>=0.1.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.1.10)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (2.0.8)\n",
      "Requirement already satisfied: future in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (1.0.0)\n",
      "Requirement already satisfied: pydot in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.0.4)\n",
      "Requirement already satisfied: omegaconf>=2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (2.2.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath>=0.1.2->detectron2) (4.13.2)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath>=0.1.2->detectron2) (3.1.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from omegaconf>=2->detectron2) (4.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (6.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2) (3.0.2)\n",
      "Requirement already satisfied: layoutparser in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (2.2.5)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (4.11.0.86)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (1.15.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (2.2.3)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (9.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (6.0.2)\n",
      "Requirement already satisfied: iopath in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (0.1.10)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (0.11.6)\n",
      "Requirement already satisfied: pdf2image in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath->layoutparser) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath->layoutparser) (4.13.2)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath->layoutparser) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pandas->layoutparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pandas->layoutparser) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pandas->layoutparser) (2025.2)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfplumber->layoutparser) (20250327)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfplumber->layoutparser) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber->layoutparser) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber->layoutparser) (44.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->layoutparser) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber->layoutparser) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber->layoutparser) (2.22)\n",
      "Requirement already satisfied: pytesseract in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pytesseract) (9.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch \n",
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2'\n",
    "!pip install -U layoutparser\n",
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import libraries\n",
    "import layoutparser as lp\n",
    "import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "model = lp.Detectron2LayoutModel(\n",
    "            config_path ='lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config', # In model catalog\n",
    "            label_map   ={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}, # In model`label_map`\n",
    "            extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8] # Optional\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Access images and PDFs from local directory\n",
    "image_dir = \"./research paper\"  # ðŸ“‚ Change this to your folder\n",
    "image_paths = glob.glob(os.path.join(image_dir, \"*.png\")) + \\\n",
    "              glob.glob(os.path.join(image_dir, \"*.jpg\")) + \\\n",
    "              glob.glob(os.path.join(image_dir, \"*.jpeg\"))\n",
    "\n",
    "pdf_paths = glob.glob(os.path.join(image_dir, \"*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert PDFs to images (each page = 1 jpg)\n",
    "from pdf2image import convert_from_path\n",
    "for pdf_path in pdf_paths:\n",
    "    pages = convert_from_path(pdf_path, dpi=300)  # Convert each page to an image\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    for idx, page in enumerate(pages):\n",
    "        img_save_path = os.path.join(image_dir, f\"{base_name}_page{idx+1}.jpg\")\n",
    "        page.save(img_save_path, \"JPEG\")  # Save each page as JPG\n",
    "        image_paths.append(img_save_path)  # Add new JPGs to processing list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page11_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page2_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page7_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page19_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page14_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page13_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page5_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page16_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page21_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page8_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page6_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page18_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page22_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page15_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page10_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page3_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page4_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page17_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page20_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page9_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page12_page_1.jpg!\n",
      "âœ… Exported structured JSON and boxed image for Multimodal Healthcare AI Identifying and Designing Clinically Relevant Vision-Language Applications for Radiology-page1_page_1.jpg!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image and ensure RGB format\n",
    "    image_np = np.array(image)  # Convert PIL image to numpy array for LayoutParser\n",
    "    layout = model.detect(image_np)  # Perform layout detection\n",
    "\n",
    "    draw = ImageDraw.Draw(image)  # Prepare to draw bounding boxes on the image\n",
    "    results = []  # Store extracted structured data here\n",
    "\n",
    "    for block in layout:\n",
    "        # Crop each detected block from the full image\n",
    "        segment_image = block.crop_image(image_np)\n",
    "\n",
    "        # Perform OCR on the cropped segment\n",
    "        text = ocr_agent.detect(segment_image)\n",
    "\n",
    "        # Draw bounding box around detected block\n",
    "        x_1, y_1, x_2, y_2 = block.coordinates\n",
    "        draw.rectangle([x_1, y_1, x_2, y_2], outline=\"red\", width=3)  # Red rectangle\n",
    "        draw.text((x_1, y_1 - 10), block.type, fill=\"red\")  # Write block type above box\n",
    "\n",
    "        # Save the block's structured information\n",
    "        results.append({\n",
    "            \"type\": block.type,\n",
    "            \"text\": text,\n",
    "            \"bounding_box\": block.coordinates\n",
    "        })\n",
    "\n",
    "    # Save extracted results as JSON\n",
    "    base_name = os.path.basename(image_path)\n",
    "    base_no_ext = os.path.splitext(base_name)[0]\n",
    "\n",
    "    json_path = os.path.join(image_dir, base_no_ext + \"_output.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)  # Write formatted JSON\n",
    "\n",
    "    # Save the image with bounding boxes\n",
    "    boxed_image_path = os.path.join(image_dir, base_no_ext + \"_boxed.png\")\n",
    "    image.save(boxed_image_path)\n",
    "\n",
    "    print(f\"âœ… Exported structured JSON and boxed image for {base_name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup folder structure\n",
    "\n",
    "remember to ensure the project folder loooks like this:\n",
    "```\n",
    "project\n",
    "â”œâ”€â”€ data\n",
    "â”‚   â”œâ”€â”€ images\n",
    "â”‚   â””â”€â”€ annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset 'doclaynet_train' is already registered!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdetectron2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_coco_instances\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Register the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mregister_coco_instances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoclaynet_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./datasets/DocLayNet/annotations/train.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./datasets/DocLayNet/images\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m register_coco_instances(\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdoclaynet_val\u001b[39m\u001b[33m\"\u001b[39m, {}, \n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m./datasets/DocLayNet/annotations/val.json\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m./datasets/DocLayNet/images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/data/datasets/coco.py:489\u001b[39m, in \u001b[36mregister_coco_instances\u001b[39m\u001b[34m(name, metadata, json_file, image_root)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_root, (\u001b[38;5;28mstr\u001b[39m, os.PathLike)), image_root\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# 1. register a function which returns dicts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mDatasetCatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_coco_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# 2. Optionally, add metadata about this dataset,\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# since they might be useful in evaluation, visualization or logging\u001b[39;00m\n\u001b[32m    493\u001b[39m MetadataCatalog.get(name).set(\n\u001b[32m    494\u001b[39m     json_file=json_file, image_root=image_root, evaluator_type=\u001b[33m\"\u001b[39m\u001b[33mcoco\u001b[39m\u001b[33m\"\u001b[39m, **metadata\n\u001b[32m    495\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/data/catalog.py:37\u001b[39m, in \u001b[36m_DatasetCatalog.register\u001b[39m\u001b[34m(self, name, func)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m    name (str): the name that identifies a dataset, e.g. \"coco_2014_train\".\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    func (callable): a callable which takes no arguments and returns a list of dicts.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m        It must return the same results if called multiple times.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func), \u001b[33m\"\u001b[39m\u001b[33mYou must register a function with `DatasetCatalog.register`!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is already registered!\u001b[39m\u001b[33m\"\u001b[39m.format(name)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m[name] = func\n",
      "\u001b[31mAssertionError\u001b[39m: Dataset 'doclaynet_train' is already registered!"
     ]
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# Register the dataset\n",
    "register_coco_instances(\n",
    "    \"doclaynet_train\", {}, \n",
    "    \"./datasets/DocLayNet/annotations/train.json\", \n",
    "    \"./datasets/DocLayNet/images\"\n",
    ")\n",
    "\n",
    "register_coco_instances(\n",
    "    \"doclaynet_val\", {}, \n",
    "    \"./datasets/DocLayNet/annotations/val.json\", \n",
    "    \"./datasets/DocLayNet/images\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))  # Base config\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"doclaynet_train\",)\n",
    "cfg.DATASETS.TEST = (\"doclaynet_val\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"  # Pretrained COCO weights\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # Lower learning rate for fine-tuning\n",
    "cfg.SOLVER.MAX_ITER = 5000    # Adjust based on your dataset size\n",
    "cfg.SOLVER.STEPS = []         # No learning rate decay\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 11  # DocLayNet has 11 classes\n",
    "\n",
    "# Force CPU usage (no GPU on macOS)\n",
    "cfg.MODEL.DEVICE = \"cpu\"  # This ensures the model runs on CPU\n",
    "\n",
    "# Output directory\n",
    "cfg.OUTPUT_DIR = \"./output_doclaynet\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/28 00:21:00 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=12, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[04/28 00:21:04 d2.data.datasets.coco]: \u001b[0mLoading ./datasets/DocLayNet/annotations/train.json takes 4.08 seconds.\n",
      "\u001b[32m[04/28 00:21:04 d2.data.datasets.coco]: \u001b[0mLoaded 69375 images in COCO format from ./datasets/DocLayNet/annotations/train.json\n",
      "\u001b[32m[04/28 00:21:06 d2.data.build]: \u001b[0mRemoved 272 images with no usable annotations. 69103 images left.\n",
      "\u001b[32m[04/28 00:21:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[04/28 00:21:06 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[04/28 00:21:06 d2.data.common]: \u001b[0mSerializing 69103 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/28 00:21:06 d2.data.common]: \u001b[0mSerialized dataset takes 142.97 MiB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from detectron2.engine import HookBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2.engine import HookBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TQDMWithLossHook(HookBase):\n",
    "    def before_train(self):\n",
    "        self.pbar = tqdm(total=self.trainer.max_iter, desc=\"Training Progress\", unit=\"iter\")\n",
    "\n",
    "    def after_step(self):\n",
    "        # Get latest logs from storage\n",
    "        storage = self.trainer.storage\n",
    "        loss_dict = storage.latest()  # latest() returns a dict of metrics\n",
    "\n",
    "        # Safely get total_loss if it exists\n",
    "        loss_value = loss_dict.get('total_loss', None)\n",
    "\n",
    "        # Update progress bar\n",
    "        if loss_value is not None:\n",
    "            self.pbar.set_postfix(loss=float(loss_value))\n",
    "        else:\n",
    "            self.pbar.set_postfix(loss=\"N/A\")\n",
    "\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def after_train(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "# Then after you create your trainer:\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.model = model  # attach the model (fine-tuned one)\n",
    "\n",
    "# Add the tqdm hook\n",
    "trainer.register_hooks([TQDMWithLossHook()])\n",
    "\n",
    "# Start training\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (after training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Load your fine-tuned model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # Set threshold for this model\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Test on new image\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "im = cv2.imread(\"./datasets/DocLayNet/images/example.png\")\n",
    "\n",
    "outputs = predictor(im)\n",
    "\n",
    "v = Visualizer(im[:, :, ::-1], scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2.imshow(\"result\", out.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutparser_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
