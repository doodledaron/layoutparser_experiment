{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: Pillow==9.5.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (9.5.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from jinja2->torch==2.7.0->torchvision) (3.0.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting detectron2\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.4) to /private/var/folders/57/0byy2pcx5fnckzr0s35zly7w0000gn/T/pip-install-v8bf6ywn/detectron2_c039ca61bf954ce19bbfc9ff303012cb\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /private/var/folders/57/0byy2pcx5fnckzr0s35zly7w0000gn/T/pip-install-v8bf6ywn/detectron2_c039ca61bf954ce19bbfc9ff303012cb\n",
      "  Running command git checkout -q 4aca4bdaa9ad48b8e91d7520e0d0815bb8ca0fb1\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 4aca4bdaa9ad48b8e91d7520e0d0815bb8ca0fb1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.0.1)\n",
      "Requirement already satisfied: Pillow>=7.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (9.5.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.1.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.10.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (4.67.1)\n",
      "Requirement already satisfied: tensorboard in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (2.19.0)\n",
      "Requirement already satisfied: fvcore<0.1.4,>=0.1.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.1.3.post20210317)\n",
      "Requirement already satisfied: iopath>=0.1.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (0.1.10)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (2.0.8)\n",
      "Requirement already satisfied: future in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (1.0.0)\n",
      "Requirement already satisfied: pydot in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (3.0.4)\n",
      "Requirement already satisfied: omegaconf>=2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from detectron2) (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (2.2.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (6.0.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath>=0.1.2->detectron2) (4.13.2)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath>=0.1.2->detectron2) (3.1.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from omegaconf>=2->detectron2) (4.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from matplotlib->detectron2) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (6.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard->detectron2) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2) (3.0.2)\n",
      "Requirement already satisfied: layoutparser in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (2.2.5)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (4.11.0.86)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (1.15.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (2.2.3)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (9.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (6.0.2)\n",
      "Requirement already satisfied: iopath in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (0.1.10)\n",
      "Requirement already satisfied: pdfplumber in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (0.11.6)\n",
      "Requirement already satisfied: pdf2image in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from layoutparser) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath->layoutparser) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath->layoutparser) (4.13.2)\n",
      "Requirement already satisfied: portalocker in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from iopath->layoutparser) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pandas->layoutparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pandas->layoutparser) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pandas->layoutparser) (2025.2)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfplumber->layoutparser) (20250327)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfplumber->layoutparser) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber->layoutparser) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pdfminer.six==20250327->pdfplumber->layoutparser) (44.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->layoutparser) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber->layoutparser) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber->layoutparser) (2.22)\n",
      "Requirement already satisfied: pytesseract in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from pytesseract) (9.5.0)\n",
      "Requirement already satisfied: tensorboard in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (2.2.5)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (6.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install \"Pillow==9.5.0\" #use a downgrade version of PIL\n",
    "!pip install torchvision\n",
    "!pip install torch \n",
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2'\n",
    "!pip install -U layoutparser\n",
    "!pip install pytesseract\n",
    "!pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Importing (from the publaynet dataset pretrained model) -> FOR INITIAL INFERENCE ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "model = lp.Detectron2LayoutModel(\n",
    "            config_path ='lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config', # In model catalog\n",
    "            label_map   ={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}, # In model`label_map`\n",
    "            extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8] # Optional\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NO NEEDED!! )Remap COCO -> as by DEFAULT the category_ids start from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model is pretrained on PubLayNet, we need to convert the annotations to the PubLayNet format -> labels have start with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remap_coco.py\n",
    "# import json\n",
    "\n",
    "# # old→new mapping\n",
    "# MAPPING = {1: 0, 2: 1, 3: 2, 4: 3}\n",
    "\n",
    "# def remap_coco(input_json, output_json):\n",
    "#     with open(input_json, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # remap categories\n",
    "#     for cat in data['categories']:\n",
    "#         cat['id'] = MAPPING[cat['id']]\n",
    "\n",
    "#     # remap each annotation\n",
    "#     for ann in data['annotations']:\n",
    "#         ann['category_id'] = MAPPING[ann['category_id']]\n",
    "\n",
    "#     # write out\n",
    "#     with open(output_json, 'w') as f:\n",
    "#         json.dump(data, f, indent=2)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     remap_coco(\"datasets/phase_1_training_minimal/annotations/train_phase_1_minimal_v2.json\", \"datasets/phase_1_training_minimal/annotations/train_phase_1_minimal_v2_remapped.json\")\n",
    "#     remap_coco(\"datasets/phase_1_training_minimal/annotations/val_phase_1_minimal_v2.json\",   \"datasets/phase_1_training_minimal/annotations/val_phase_1_minimal_v2_remapped.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "# ── Register your NEW 4-class splits ───────────────────────────────\n",
    "register_coco_instances(\n",
    "    \"train_phase_1_minimal_v2_remapped\", {}, \n",
    "    \"./datasets/phase_1_training_minimal/annotations/train_phase_1_minimal_v2_remapped.json\",\n",
    "    \"./datasets/phase_1_training_minimal/images\"\n",
    ")\n",
    "register_coco_instances(\n",
    "    \"val_phase_1_minimal_v2_remapped\",   {}, \n",
    "    \"./datasets/phase_1_training_minimal/annotations/val_phase_1_minimal_v2_remapped.json\",\n",
    "    \"./datasets/phase_1_training_minimal/images\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just to check your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image root: ./datasets/phase_1_training_minimal/images\n",
      "Annotation file: ./datasets/phase_1_training_minimal/annotations/val_phase_1_minimal_v2_remapped.json\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# List all registered datasets\n",
    "# print(\"Registered datasets:\", DatasetCatalog.list())\n",
    "\n",
    "metadata = MetadataCatalog.get(\"val_phase_1_minimal_v2_remapped\")\n",
    "print(\"Image root:\", metadata.image_root)\n",
    "print(\"Annotation file:\", metadata.json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import os\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))  # Base config\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"train_phase_1_minimal_v2_remapped\",)\n",
    "cfg.DATASETS.TEST = (\"val_phase_1_minimal_v2_remapped\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 8 \n",
    "# NOTE dont change weights -> USE PUBLAYNET PRETRAINED MODEL\n",
    "cfg.MODEL.WEIGHTS = \"pretrained_models/model_final.pth\"  \n",
    "cfg.SOLVER.LOG_PERIOD = 50 # Log every 50 iterations\n",
    "\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # Lower learning rate for fine-tuning\n",
    "\n",
    "# NOTE rule of thumb for count of iterations: num_images / IMS_PER_BATCH \n",
    "# if you have 40 images and IMS_PER_BATCH = 4, and you want to train for 20 epochs, then MAX_ITER = 40 / 4  = 200\n",
    "\"\"\"\n",
    "I have 64 images in train_phase_1_minimal_v2_remapped:\n",
    "Assuming IMS_PER_BATCH = 4 and you want to train for 20 epochs:\n",
    "\n",
    "- Iterations per epoch: 64 / 4 = 16\n",
    "- For 20 epochs: 16 * 20 = 320\n",
    "\"\"\"\n",
    "\n",
    "cfg.SOLVER.MAX_ITER = 320    # Adjust based on your dataset size\n",
    "cfg.SOLVER.STEPS = []         # No learning rate decay\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  \n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8\n",
    "\n",
    "# Force CPU usage (no GPU on macOS)\n",
    "cfg.MODEL.DEVICE = \"cpu\"  # This ensures the model runs on CPU\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4 #IMPORTANT - number of classes\n",
    "\n",
    "# Output directory -> NOTE u can change this\n",
    "cfg.OUTPUT_DIR = \"./training_results/training3_output_phase_1_minimal_detectron_ready\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (dont change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:34:45 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/05 19:34:45 d2.data.datasets.coco]: \u001b[0mLoaded 63 images in COCO format from ./datasets/phase_1_training_minimal/annotations/train_phase_1_minimal_v2.json\n",
      "\u001b[32m[05/05 19:34:45 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 63 images left.\n",
      "\u001b[32m[05/05 19:34:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[05/05 19:34:45 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[05/05 19:34:45 d2.data.common]: \u001b[0mSerializing 63 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/05 19:34:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.04 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (7, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (7,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (24, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (24,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:34:45 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/320 [00:00<?, ?iter/s]/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Training:   6%|▌         | 19/320 [01:27<22:37,  4.51s/iter, loss=2.54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:36:18 d2.utils.events]: \u001b[0m eta: 0:23:06  iter: 19  total_loss: 2.67  loss_cls: 1.501  loss_box_reg: 0.855  loss_rpn_cls: 0.09199  loss_rpn_loc: 0.1878  time: 4.5002  data_time: 0.0737  lr: 4.9953e-06  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 39/320 [02:59<20:37,  4.40s/iter, loss=2.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:37:49 d2.utils.events]: \u001b[0m eta: 0:21:27  iter: 39  total_loss: 2.482  loss_cls: 1.277  loss_box_reg: 0.8536  loss_rpn_cls: 0.1008  loss_rpn_loc: 0.228  time: 4.5173  data_time: 0.0013  lr: 9.9902e-06  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 59/320 [04:29<20:18,  4.67s/iter, loss=2]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:39:19 d2.utils.events]: \u001b[0m eta: 0:19:59  iter: 59  total_loss: 2.116  loss_cls: 0.9425  loss_box_reg: 0.8638  loss_rpn_cls: 0.05858  loss_rpn_loc: 0.2023  time: 4.5111  data_time: 0.0013  lr: 1.4985e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 79/320 [06:00<17:44,  4.42s/iter, loss=2.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:40:51 d2.utils.events]: \u001b[0m eta: 0:18:33  iter: 79  total_loss: 1.886  loss_cls: 0.765  loss_box_reg: 0.8353  loss_rpn_cls: 0.04882  loss_rpn_loc: 0.1969  time: 4.5408  data_time: 0.0013  lr: 1.998e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 99/320 [07:31<16:47,  4.56s/iter, loss=1.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:42:21 d2.utils.events]: \u001b[0m eta: 0:16:55  iter: 99  total_loss: 1.796  loss_cls: 0.6924  loss_box_reg: 0.8449  loss_rpn_cls: 0.04306  loss_rpn_loc: 0.176  time: 4.5289  data_time: 0.0013  lr: 2.4975e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 119/320 [09:00<15:03,  4.49s/iter, loss=1.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:43:50 d2.utils.events]: \u001b[0m eta: 0:15:17  iter: 119  total_loss: 1.702  loss_cls: 0.6334  loss_box_reg: 0.8373  loss_rpn_cls: 0.04382  loss_rpn_loc: 0.1539  time: 4.5151  data_time: 0.0012  lr: 2.997e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 139/320 [10:27<12:41,  4.21s/iter, loss=1.51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:45:17 d2.utils.events]: \u001b[0m eta: 0:13:42  iter: 139  total_loss: 1.64  loss_cls: 0.6139  loss_box_reg: 0.8119  loss_rpn_cls: 0.04227  loss_rpn_loc: 0.1812  time: 4.4958  data_time: 0.0012  lr: 3.4965e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 159/320 [11:54<11:56,  4.45s/iter, loss=1.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:46:45 d2.utils.events]: \u001b[0m eta: 0:12:06  iter: 159  total_loss: 1.637  loss_cls: 0.5821  loss_box_reg: 0.83  loss_rpn_cls: 0.04502  loss_rpn_loc: 0.193  time: 4.4782  data_time: 0.0012  lr: 3.996e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 179/320 [13:21<10:35,  4.50s/iter, loss=1.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:48:11 d2.utils.events]: \u001b[0m eta: 0:10:24  iter: 179  total_loss: 1.528  loss_cls: 0.5676  loss_box_reg: 0.7877  loss_rpn_cls: 0.03975  loss_rpn_loc: 0.1741  time: 4.4629  data_time: 0.0012  lr: 4.4955e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 199/320 [14:51<09:32,  4.73s/iter, loss=0.973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:49:42 d2.utils.events]: \u001b[0m eta: 0:08:56  iter: 199  total_loss: 1.549  loss_cls: 0.5341  loss_box_reg: 0.776  loss_rpn_cls: 0.03582  loss_rpn_loc: 0.1725  time: 4.4678  data_time: 0.0013  lr: 4.995e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 219/320 [16:19<07:30,  4.46s/iter, loss=1.44] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:51:10 d2.utils.events]: \u001b[0m eta: 0:07:26  iter: 219  total_loss: 1.448  loss_cls: 0.474  loss_box_reg: 0.7412  loss_rpn_cls: 0.03476  loss_rpn_loc: 0.1803  time: 4.4617  data_time: 0.0013  lr: 5.4945e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 239/320 [17:53<06:39,  4.94s/iter, loss=1.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:52:43 d2.utils.events]: \u001b[0m eta: 0:05:58  iter: 239  total_loss: 1.401  loss_cls: 0.4929  loss_box_reg: 0.6946  loss_rpn_cls: 0.03311  loss_rpn_loc: 0.187  time: 4.4775  data_time: 0.0013  lr: 5.994e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 259/320 [19:30<04:49,  4.74s/iter, loss=1.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:54:21 d2.utils.events]: \u001b[0m eta: 0:04:31  iter: 259  total_loss: 1.329  loss_cls: 0.4466  loss_box_reg: 0.6633  loss_rpn_cls: 0.03483  loss_rpn_loc: 0.1693  time: 4.5094  data_time: 0.0014  lr: 6.4935e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 279/320 [21:03<03:13,  4.72s/iter, loss=1.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:55:53 d2.utils.events]: \u001b[0m eta: 0:03:01  iter: 279  total_loss: 1.264  loss_cls: 0.4254  loss_box_reg: 0.6332  loss_rpn_cls: 0.03518  loss_rpn_loc: 0.1714  time: 4.5189  data_time: 0.0014  lr: 6.993e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 299/320 [22:37<01:40,  4.80s/iter, loss=0.97] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:57:28 d2.utils.events]: \u001b[0m eta: 0:01:30  iter: 299  total_loss: 1.202  loss_cls: 0.4074  loss_box_reg: 0.5704  loss_rpn_cls: 0.04136  loss_rpn_loc: 0.1687  time: 4.5320  data_time: 0.0013  lr: 7.4925e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 319/320 [24:05<00:04,  4.22s/iter, loss=1.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:58:55 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 319  total_loss: 1.123  loss_cls: 0.3845  loss_box_reg: 0.5273  loss_rpn_cls: 0.0376  loss_rpn_loc: 0.1787  time: 4.5215  data_time: 0.0012  lr: 7.992e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [24:10<00:00,  4.17s/iter, loss=0.944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/05 19:58:55 d2.engine.hooks]: \u001b[0mOverall training speed: 318 iterations in 0:23:57 (4.5215 s / it)\n",
      "\u001b[32m[05/05 19:58:55 d2.engine.hooks]: \u001b[0mTotal training time: 0:23:58 (0:00:00 on hooks)\n",
      "\u001b[32m[05/05 19:58:55 d2.data.datasets.coco]: \u001b[0mLoaded 16 images in COCO format from ./datasets/phase_1_training_minimal/annotations/val_phase_1_minimal_v2.json\n",
      "\u001b[32m[05/05 19:58:55 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[05/05 19:58:55 d2.data.common]: \u001b[0mSerializing 16 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/05 19:58:55 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/05 19:58:55 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 320/320 [24:10<00:00,  4.53s/iter, loss=0.944]\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import HookBase\n",
    "from tqdm import tqdm\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import HookBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TQDMWithLossHook(HookBase):\n",
    "    def before_train(self):\n",
    "        self.pbar = tqdm(total=self.trainer.max_iter, desc=\"Training\", unit=\"iter\")\n",
    "\n",
    "    def after_step(self):\n",
    "        storage   = self.trainer.storage\n",
    "        loss_dict = storage.latest()\n",
    "        raw       = loss_dict.get(\"total_loss\", None)\n",
    "\n",
    "        # Unpack tuple if necessary\n",
    "        if isinstance(raw, (tuple, list)):\n",
    "            loss_value = raw[0]\n",
    "        else:\n",
    "            loss_value = raw\n",
    "\n",
    "        # Now it's safe to float()\n",
    "        if loss_value is not None:\n",
    "            self.pbar.set_postfix(loss=float(loss_value))\n",
    "        else:\n",
    "            self.pbar.set_postfix(loss=\"N/A\")\n",
    "\n",
    "        self.pbar.update(1)\n",
    "\n",
    "    def after_train(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "# Then after you create your trainer:\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "\n",
    "# Add the tqdm hook\n",
    "trainer.register_hooks([TQDMWithLossHook()])\n",
    "\n",
    "# Start training\n",
    "trainer.resume_or_load(resume=False) # <-- ensures a fresh load of the pretrained weights\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Build config same as training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from detectron2.config        import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine        import DefaultPredictor\n",
    "from detectron2.data          import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "))\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES       = 4\n",
    "cfg.MODEL.WEIGHTS                     = \"training_results/training3_output_phase_1_minimal_detectron_ready/model_final.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8\n",
    "cfg.MODEL.DEVICE                      = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Create predictor and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DefaultPredictor(cfg)\n",
    "metadata  = MetadataCatalog.get(\"val_phase_1_minimal_v2_remapped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Inference and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/920cb6ed__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_7_output.jpg\n"
     ]
    }
   ],
   "source": [
    "def infer_and_save(image_path, predictor, metadata, output_dir):\n",
    "    im = cv2.imread(image_path)\n",
    "    outputs = predictor(im)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.2)\n",
    "    out = v.draw_instance_predictions(instances)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    save_path = os.path.join(output_dir, f\"{base}_output.jpg\")\n",
    "    cv2.imwrite(save_path, out.get_image()[:, :, ::-1])\n",
    "    print(\"Saved:\", save_path)\n",
    "\n",
    "# Usage\n",
    "infer_and_save(\n",
    "    \"/Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/datasets/phase_1_training_minimal/images/920cb6ed__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_7.png\",\n",
    "    predictor,\n",
    "    metadata,\n",
    "    \"/Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference \n",
    "1. Inferene the images available in the folder\n",
    "2. Based on the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/920cb6ed__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_7_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/78affaa4__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_8_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/dbd47ee1__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_19_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/bc80bfcb__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab1_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/e440996b__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_6_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/687ee17c__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_4_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/197c0bde__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab1_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/5c330dab__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_5_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/04db7c14__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/e10fe46b__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_22_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/af55af41__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab10_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/781afd39__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_20_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/054ab297__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/64389699__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/529aaa69__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_14_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/1016b993__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_15_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/fc0b70ab__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/b5835a75__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab6_page_4_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/ed0d184d__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/8bbe432b__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_8_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/5651d474__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_17_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/e2592121__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_12_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/5c74b47f__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_5_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/249d7153__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab6_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/a55e055f__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_7_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/9b51e12b__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_10_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/9b41ad94__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_6_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/be8ed2a8__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab6_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/016c1bc4__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab3_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/fee0f8de__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/ba3ca0d4__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/4b66689e__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_8_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/93a2376c__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab3_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/335b4e18__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/19b50ef6__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab3_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/3c6d1f13__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_9_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/2f991c8b__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_4_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/7e67224d__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_5_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/1c8a28bd__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_7_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/f85777cb__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab3_page_18_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/6acad32b__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab11_page_6_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/d6cb5c2c__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab13_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/479a0d03__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/a845941a__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_11_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/2e4ce69c__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab13_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/1f00a066__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/ecca3cef__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_13_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/cfcaa8da__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_12_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/7909717e__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_3_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/8f88ad4e__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab1_page_2_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/9145cefd__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab13_page_1_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/3afc2089__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab13_page_5_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/d345623a__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_8_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/4b95a32b__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_13_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/86707281__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_7_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/5c7f22d6__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_17_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/6d5f352e__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_9_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/d120c87e__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab13_page_4_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/01b3c57c__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_10_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/25ccf134__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_4_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/a123587d__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_15_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/c92216f7__Appellant___Appellant_s_Bundle_of_Documents_Volume_1_Tab2_page_14_output.jpg\n",
      "Saved: /Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images/f2b4ccd4__Appellant_s_Bundle_of_Documents_Volume_1_Tab6_Tab7_page_11_output.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# --- Paths (modify as needed) ---\n",
    "images_dir = \"/Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/datasets/phase_1_training_minimal/images\"\n",
    "json_path  = \"/Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/datasets/phase_1_training_minimal/annotations/train_phase_1_minimal_v2_remapped.json\"  # or val_phase_1_minimal_v2_remapped.json\n",
    "output_dir = \"/Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/training_images\"\n",
    "\n",
    "# 4) Prepare output directory for batch inference\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 5) Load list of images from JSON\n",
    "with open(json_path, 'r') as f:\n",
    "    imgs = json.load(f)[\"images\"]\n",
    "\n",
    "# 6) Run inference & save visualizations\n",
    "for img_info in imgs:\n",
    "    img_file = img_info[\"file_name\"] # get the image file name\n",
    "    img_path = os.path.join(images_dir, img_file)\n",
    "    if not os.path.exists(img_path):\n",
    "        continue\n",
    "\n",
    "    im = cv2.imread(img_path) # load image\n",
    "    outputs = predictor(im) # run inference\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    # Save the visualized image\n",
    "    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.2)\n",
    "    out = v.draw_instance_predictions(instances)\n",
    "    save_name = os.path.splitext(img_file)[0] + \"_output.jpg\"\n",
    "    save_path = os.path.join(output_dir, save_name)\n",
    "\n",
    "    cv2.imwrite(save_path, out.get_image()[:, :, ::-1])\n",
    "    print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/05 20:05:54 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[05/05 20:05:54 d2.data.datasets.coco]: \u001b[0mLoaded 16 images in COCO format from ./datasets/phase_1_training_minimal/annotations/val_phase_1_minimal_v2.json\n",
      "\u001b[32m[05/05 20:05:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[05/05 20:05:54 d2.data.common]: \u001b[0mSerializing 16 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/05 20:05:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n",
      "\u001b[32m[05/05 20:05:54 d2.evaluation.evaluator]: \u001b[0mStart inference on 16 images\n",
      "\u001b[32m[05/05 20:06:02 d2.evaluation.evaluator]: \u001b[0mInference done 11/16. 0.5720 s / img. ETA=0:00:02\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:26.003141 (2.363922 s / img per device, on 1 devices)\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:05 (0.544590 s / img per device, on 1 devices)\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.00 seconds.\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.057\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.079\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.076\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.076\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.076\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.685 | 7.868  | 6.379  |  nan  | 0.000 | 5.685 |\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[05/05 20:06:25 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    | category   | AP     |\n",
      "|:-----------|:------|:-----------|:------|:-----------|:-------|\n",
      "| Figure     | 0.000 | Table      | 0.000 | Text       | 22.742 |\n",
      "| Title      | 0.000 |            |       |            |        |\n",
      "OrderedDict([('bbox', {'AP': 5.685391940723206, 'AP50': 7.867796732439776, 'AP75': 6.379409870811642, 'APs': nan, 'APm': 0.0, 'APl': 5.685391940723206, 'AP-Figure': 0.0, 'AP-Table': 0.0, 'AP-Text': 22.741567762892824, 'AP-Title': 0.0})])\n"
     ]
    }
   ],
   "source": [
    "# Create evaluator and data loader for validation set\n",
    "evaluator = COCOEvaluator(\"val_phase_1_minimal_v2_remapped\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"val_phase_1_minimal_v2_remapped\")\n",
    "\n",
    "# Run inference and evaluation\n",
    "results = inference_on_dataset(trainer.model, val_loader, evaluator)\n",
    "print(results)\n",
    "\n",
    "# Save results to file\n",
    "eval_output_dir = \"/Users/doodledaron/Documents/Freelances/Leon/layoutparser_experiment/training_results/training3_output_phase_1_minimal_detectron_ready/evaluator\"\n",
    "os.makedirs(eval_output_dir, exist_ok=True)\n",
    "with open(os.path.join(eval_output_dir, \"eval_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue: Class Imbalance\n",
    "### Strategy:\n",
    "1. Freeze the backbone : cfg.MODEL.BACKBONE.FREEZE_AT = 5  # Freeze all ResNet stages (0–4)\n",
    "2. Enable RepeatFactorTrainingSampler (RFS)\n",
    "3. Increase Iterations to 1000 and Tune LR Schedule\n",
    "4. (Optional Bonus) 🔁 Use Augmentation (Drop-in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import os\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"train_phase_1_minimal_v2_remapped\",)\n",
    "cfg.DATASETS.TEST  = (\"val_phase_1_minimal_v2_remapped\",)\n",
    "\n",
    "# load the pretrained model weights from publaynet\n",
    "cfg.MODEL.WEIGHTS = \"pretrained_models/model_final.pth\"  # from PubLayNet\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR       = 0.00025\n",
    "cfg.SOLVER.MAX_ITER      = 1000 # # ~62.5 epochs at batch 4\n",
    "\"\"\"\n",
    "How it works :\n",
    "- Training starts with your base learning rate ( cfg.SOLVER.BASE_LR = 0.00025 )\n",
    "- At iteration 600, the learning rate will be multiplied by cfg.SOLVER.GAMMA = 0.1\n",
    "- At iteration 800, it will be multiplied by cfg.SOLVER.GAMMA = 0.1 again\n",
    "\n",
    "- Initial LR: 0.00025\n",
    "- At iteration 600: LR becomes 0.000025\n",
    "- At iteration 800: LR becomes 0.0000025\n",
    "\"\"\"\n",
    "cfg.SOLVER.STEPS         = (600, 800)\n",
    "cfg.SOLVER.GAMMA         = 0.1\n",
    "\n",
    "\"\"\"\n",
    "- First 100 iterations: LR increases linearly from ~0 to 0.00025\n",
    "- Helps stabilize early training\n",
    "\"\"\"\n",
    "cfg.SOLVER.WARMUP_ITERS  = 100\n",
    "cfg.SOLVER.WARMUP_METHOD = \"linear\"\n",
    "\n",
    "cfg.SOLVER.LOG_PERIOD    = 50\n",
    "cfg.DATALOADER.NUM_WORKERS = 8  # use lower if CPU limited\n",
    "\n",
    "\"\"\"\n",
    "- Helps with class imbalance by oversampling rare categories\n",
    "- Images with rare categories appear more frequently\n",
    "\"\"\"\n",
    "cfg.DATALOADER.SAMPLER_TRAIN = \"RepeatFactorTrainingSampler\"\n",
    "\n",
    "\"\"\"\n",
    "- Setting REPEAT_THRESHOLD = 0.1 means any category that appears in less than 10% of your images will be oversampled\n",
    "- This helps balance your training data where \"Text\" is dominant but \"Figure\", \"Table\", and \"Title\" are rare\n",
    "\"\"\"\n",
    "cfg.DATALOADER.REPEAT_THRESHOLD = 0.1\n",
    "\n",
    "# freeze the first 5 layers of the backbone and train the remaining layers\n",
    "# helps with faster training and less overfitting\n",
    "# also helps with generalization as it reduces the risk of overfitting to the training data\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 5\n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.DEVICE = \"cpu\"\n",
    "\n",
    "cfg.OUTPUT_DIR = \"./training_results/training3_output_phase_1_minimal_detectron_ready_solving_class_imbalance\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Augmented Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "class AugmentedTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        aug = [\n",
    "            T.RandomRotation(angle=[-5, 5]),\n",
    "            T.RandomBrightness(0.9, 1.1),\n",
    "            T.RandomFlip(horizontal=True, vertical=False)\n",
    "        ]\n",
    "        mapper = DatasetMapper(cfg, is_train=True, augmentations=aug)\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Add Progress Bar Hook (Optional but Nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import HookBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TQDMWithLossHook(HookBase):\n",
    "    def before_train(self):\n",
    "        self.pbar = tqdm(total=self.trainer.max_iter, desc=\"Training\", unit=\"iter\")\n",
    "    def after_step(self):\n",
    "        storage = self.trainer.storage\n",
    "        loss_dict = storage.latest()\n",
    "        loss = loss_dict.get(\"total_loss\", None)\n",
    "        if isinstance(loss, (tuple, list)):\n",
    "            loss = loss[0]\n",
    "        self.pbar.set_postfix(loss=float(loss) if loss else \"N/A\")\n",
    "        self.pbar.update(1)\n",
    "    def after_train(self):\n",
    "        self.pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/06 15:33:22 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/06 15:33:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [RandomRotation(angle=[-5, 5]), RandomBrightness(intensity_min=0.9, intensity_max=1.1), RandomFlip()]\n",
      "\u001b[32m[05/06 15:33:22 d2.data.datasets.coco]: \u001b[0mLoaded 63 images in COCO format from ./datasets/phase_1_training_minimal/annotations/train_phase_1_minimal_v2_remapped.json\n",
      "\u001b[32m[05/06 15:33:22 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 63 images left.\n",
      "\u001b[32m[05/06 15:33:22 d2.data.build]: \u001b[0mUsing training sampler RepeatFactorTrainingSampler\n",
      "\u001b[32m[05/06 15:33:22 d2.data.common]: \u001b[0mSerializing 63 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/06 15:33:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.04 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (7, 1024) in the checkpoint but (5, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (7,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (24, 1024) in the checkpoint but (16, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (24,) in the checkpoint but (16,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/06 15:33:22 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1000 [00:00<?, ?iter/s]/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/structures/boxes.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)\n",
      "/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Training:   1%|          | 8/1000 [43:54<202:42:22, 735.63s/iter, loss=3.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/06 16:17:22 d2.engine.hooks]: \u001b[0mOverall training speed: 6 iterations in 0:06:46 (67.7240 s / it)\n",
      "\u001b[32m[05/06 16:17:22 d2.engine.hooks]: \u001b[0mTotal training time: 0:06:46 (0:00:00 on hooks)\n",
      "\u001b[32m[05/06 16:17:22 d2.utils.events]: \u001b[0m eta: 18:13:52  iter: 8  total_loss: 3.524  loss_cls: 1.894  loss_box_reg: 0.4738  loss_rpn_cls: 0.3745  loss_rpn_loc: 0.8135  time: 66.8219  data_time: 0.1868  lr: 1.7733e-05  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 8/1000 [43:59<90:55:44, 329.98s/iter, loss=3.5] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m trainer.register_hooks([TQDMWithLossHook()])\n\u001b[32m      3\u001b[39m trainer.resume_or_load(resume=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/engine/defaults.py:431\u001b[39m, in \u001b[36mDefaultTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    425\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03m    Run training.\u001b[39;00m\n\u001b[32m    427\u001b[39m \n\u001b[32m    428\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    429\u001b[39m \u001b[33;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.cfg.TEST.EXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm.is_main_process():\n\u001b[32m    433\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    434\u001b[39m             \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_last_eval_results\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    435\u001b[39m         ), \u001b[33m\"\u001b[39m\u001b[33mNo evaluation results obtained during training!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/engine/train_loop.py:138\u001b[39m, in \u001b[36mTrainerBase.train\u001b[39m\u001b[34m(self, start_iter, max_iter)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter, max_iter):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.before_step()\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m.after_step()\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/engine/defaults.py:441\u001b[39m, in \u001b[36mDefaultTrainer.run_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    440\u001b[39m     \u001b[38;5;28mself\u001b[39m._trainer.iter = \u001b[38;5;28mself\u001b[39m.iter\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/engine/train_loop.py:232\u001b[39m, in \u001b[36mSimpleTrainer.run_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    227\u001b[39m data_time = time.perf_counter() - start\n\u001b[32m    229\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[33;03mIf you want to do something with the losses, you can wrap the model.\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m loss_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m losses = \u001b[38;5;28msum\u001b[39m(loss_dict.values())\n\u001b[32m    235\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m \u001b[33;03mIf you need to accumulate gradients or do something similar, you can\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03mwrap the optimizer with your custom `zero_grad()` method.\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/modeling/meta_arch/rcnn.py:157\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, batched_inputs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m     gt_instances = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m     proposals, proposal_losses = \u001b[38;5;28mself\u001b[39m.proposal_generator(images, features, gt_instances)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/modeling/backbone/fpn.py:126\u001b[39m, in \u001b[36mFPN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m \u001b[33;03m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     bottom_up_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbottom_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     results = []\n\u001b[32m    128\u001b[39m     prev_features = \u001b[38;5;28mself\u001b[39m.lateral_convs[\u001b[32m0\u001b[39m](bottom_up_features[\u001b[38;5;28mself\u001b[39m.in_features[-\u001b[32m1\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/modeling/backbone/resnet.py:448\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    446\u001b[39m     outputs[\u001b[33m\"\u001b[39m\u001b[33mstem\u001b[39m\u001b[33m\"\u001b[39m] = x\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.stage_names, \u001b[38;5;28mself\u001b[39m.stages):\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     x = \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._out_features:\n\u001b[32m    450\u001b[39m         outputs[name] = x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/modeling/backbone/resnet.py:198\u001b[39m, in \u001b[36mBottleneckBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    195\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m    196\u001b[39m out = F.relu_(out)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m out = F.relu_(out)\n\u001b[32m    201\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv3(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/layoutparser_experiment/lib/python3.11/site-packages/detectron2/layers/wrappers.py:84\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.numel() == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     79\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/12013\u001b[39;00m\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m     81\u001b[39m             \u001b[38;5;28mself\u001b[39m.norm, torch.nn.SyncBatchNorm\n\u001b[32m     82\u001b[39m         ), \u001b[33m\"\u001b[39m\u001b[33mSyncBatchNorm does not support empty inputs!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm(x)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = AugmentedTrainer(cfg)\n",
    "trainer.register_hooks([TQDMWithLossHook()])\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutparser_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
