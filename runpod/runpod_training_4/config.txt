import os
# ── TUNE THE CUDA ALLOCATOR ─────────────────────────────────────────
# break large reserved chunks into ≤128 MB pieces to reduce fragmentation
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

import torch
# also enforce it in‐process
torch.backends.cuda.max_split_size_mb = 128

from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer, HookBase
from detectron2.data import DatasetMapper, build_detection_train_loader
import detectron2.data.transforms as T
from tqdm import tqdm

# ── 1) BUILD CONFIG ──────────────────────────────────────────────────
cfg = get_cfg()
cfg.merge_from_file(
    model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
)
cfg.DATASETS.TRAIN = ("train_phase_1_minimal_v2_remapped",)
cfg.DATASETS.TEST  = ("val_phase_1_minimal_v2_remapped",)
cfg.MODEL.WEIGHTS = "/workspace/layoutparser/pretrained_models/model_final.pth"
cfg.MODEL.DEVICE  = "cuda"

# ── MEMORY SAVING SETTINGS ────────────────────────────────────────────
# 1) reduce how many images we process at once
cfg.SOLVER.IMS_PER_BATCH = 4               # ↓ from 8 → 4 (or even 2)

# 2) reduce proposal‐sampling per image
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32  # ↓ from 128 → 32

# 3) freeze early backbone layers to cut gradient buffers (optional)
cfg.MODEL.BACKBONE.FREEZE_AT = 2

# 4) mixed‐precision is already on, keep it
cfg.SOLVER.AMP.ENABLED = True

# rest of your solver/hook settings...
cfg.SOLVER.BASE_LR      = 1e-4
cfg.SOLVER.MAX_ITER     = 1500
cfg.SOLVER.WARMUP_ITERS = 100
cfg.SOLVER.LRSCHEDULER_NAME = "WarmupCosineLR"
cfg.SOLVER.STEPS       = ()
cfg.SOLVER.LOG_PERIOD  = 50

cfg.DATALOADER.SAMPLER_TRAIN    = "RepeatFactorTrainingSampler"
cfg.DATALOADER.REPEAT_THRESHOLD = 0.1
cfg.DATALOADER.NUM_WORKERS      = 4

cfg.OUTPUT_DIR = "./training_results/training4_output_gpu_optimized_full_backbone_unfreeze_cosine_lr_trainer"
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)


# ── 2) CUSTOM TRAINER WITH AUGMENTATION ──────────────────────────────
class CosineTrainer(DefaultTrainer):
    @classmethod
    def build_train_loader(cls, cfg):
        aug = [
            T.RandomFlip(),
            T.RandomBrightness(0.9, 1.1),
        ]
        mapper = DatasetMapper(cfg, is_train=True, augmentations=aug)
        return build_detection_train_loader(cfg, mapper=mapper)


# ── 3) PROGRESS BAR HOOK ──────────────────────────────────────────────
class TQDMHookB(HookBase):
    def before_train(self):
        self.pbar = tqdm(total=self.trainer.max_iter, desc="Option B", unit="iter")
    def after_step(self):
        # soft-clear any unused cache each step to help fragmentation
        torch.cuda.empty_cache()
        loss = self.trainer.storage.latest().get("total_loss", 0)
        if isinstance(loss, (tuple, list)): loss = loss[0]
        self.pbar.set_postfix(loss=float(loss))
        self.pbar.update(1)
    def after_train(self):
        self.pbar.close()


# ── 4) LAUNCH TRAINING ────────────────────────────────────────────────
if __name__ == "__main__":
    trainer = CosineTrainer(cfg)
    trainer.register_hooks([TQDMHookB()])

    # hard-clear cache right before we start
    torch.cuda.empty_cache()

    trainer.resume_or_load(resume=False)
    trainer.train()
